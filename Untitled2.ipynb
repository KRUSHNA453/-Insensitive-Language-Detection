{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1dSNURXajJzNhO6oSTYI6ARr5dPsBKI4B","authorship_tag":"ABX9TyP9tq0qSGsbWH+ofUsqZXiX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"tUb52x_OLOAI","executionInfo":{"status":"ok","timestamp":1684067049594,"user_tz":-330,"elapsed":2112,"user":{"displayName":"Sangeeth Ajith","userId":"01644790090705111252"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import copy\n","from sklearn.metrics import classification_report, confusion_matrix"]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NLP END SEM/tamil_sentiment_full.csv',sep='\\t',names=['category','text'])\n","text=df[['text']]\n","labels=df[['category']]\n","#text"],"metadata":{"id":"naH3ba8tLUp_","executionInfo":{"status":"ok","timestamp":1684067051679,"user_tz":-330,"elapsed":948,"user":{"displayName":"Sangeeth Ajith","userId":"01644790090705111252"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import re\n","import nltk\n","nltk.download('punkt', 'stopwords')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem.lancaster import LancasterStemmer\n","lancaster_stemmer = LancasterStemmer()\n","from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oFnu62dUMOhC","executionInfo":{"status":"ok","timestamp":1684067053716,"user_tz":-330,"elapsed":2039,"user":{"displayName":"Sangeeth Ajith","userId":"01644790090705111252"}},"outputId":"148f62ba-fa3d-49fc-9f8c-a08fae37ff37"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to stopwords...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["def take_data_to_shower(tweet):\n","    noises = ['URL', '@USER', '\\'ve', 'n\\'t', '\\'s', '\\'m']\n","\n","    for noise in noises:\n","        tweet = tweet.replace(noise, '')\n","\n","    return re.sub(r'[^a-zA-Z]', ' ', tweet)"],"metadata":{"id":"yuOi90xCMSZH","executionInfo":{"status":"ok","timestamp":1684067053717,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sangeeth Ajith","userId":"01644790090705111252"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def tokenize(tweet):\n","    lower_tweet = tweet.lower()\n","    return word_tokenize(lower_tweet)"],"metadata":{"id":"1PtAR4QvMX2r","executionInfo":{"status":"ok","timestamp":1684067059648,"user_tz":-330,"elapsed":1,"user":{"displayName":"Sangeeth Ajith","userId":"01644790090705111252"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def remove_stop_words(tokens):\n","    clean_tokens = []\n","    stopWords = set(stopwords.words('english'))\n","    for token in tokens:\n","        if token not in stopWords:\n","            if token.replace(' ', '') != '':\n","                if len(token) > 1:\n","                    clean_tokens.append(token)\n","    return clean_tokens"],"metadata":{"id":"UKBdMdc8McOu","executionInfo":{"status":"ok","timestamp":1684067061070,"user_tz":-330,"elapsed":1,"user":{"displayName":"Sangeeth Ajith","userId":"01644790090705111252"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def stem_and_lem(tokens):\n","    clean_tokens = []\n","    for token in tokens:\n","        token = wordnet_lemmatizer.lemmatize(token)\n","        token = lancaster_stemmer.stem(token)\n","        if len(token) > 1:\n","            clean_tokens.append(token)\n","    return clean_tokens"],"metadata":{"id":"jeJZpg0pMdzk","executionInfo":{"status":"ok","timestamp":1684067062005,"user_tz":-330,"elapsed":1,"user":{"displayName":"Sangeeth Ajith","userId":"01644790090705111252"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["clean_texts = copy.deepcopy(text)\n","tqdm.pandas(desc=\"Cleaning Data Phase I...\")\n","clean_texts['text'] = text['text'].progress_apply(take_data_to_shower)\n","\n","tqdm.pandas(desc=\"Tokenizing Data...\")\n","clean_texts['tokens'] = clean_texts['text'].progress_apply(tokenize)\n","\n","tqdm.pandas(desc=\"Cleaning Data Phase II...\")\n","clean_texts['tokens'] = clean_texts['tokens'].progress_apply(remove_stop_words)\n","\n","tqdm.pandas(desc=\"Stemming And Lemmatizing\")\n","clean_texts['tokens'] = clean_texts['tokens'].progress_apply(stem_and_lem)\n","\n","text_vector = clean_texts['tokens'].tolist()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WF84_-E_MgbQ","executionInfo":{"status":"ok","timestamp":1684067087722,"user_tz":-330,"elapsed":24298,"user":{"displayName":"Sangeeth Ajith","userId":"01644790090705111252"}},"outputId":"53b571f2-2dbf-484c-d2a6-2c0a7f9c135b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Cleaning Data Phase I...: 100%|██████████| 44020/44020 [00:02<00:00, 19661.06it/s]\n","Tokenizing Data...: 100%|██████████| 44020/44020 [00:09<00:00, 4751.03it/s] \n","Cleaning Data Phase II...: 100%|██████████| 44020/44020 [00:06<00:00, 7166.70it/s]\n","Stemming And Lemmatizing: 100%|██████████| 44020/44020 [00:06<00:00, 7048.88it/s] \n"]}]},{"cell_type":"code","source":["nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XIdCq8qVMkwu","executionInfo":{"status":"ok","timestamp":1684067087723,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sangeeth Ajith","userId":"01644790090705111252"}},"outputId":"c44f92be-fe79-4715-d34f-615ca413d639"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def tfid(text_vector):\n","    vectorizer = TfidfVectorizer()\n","    untokenized_data =[' '.join(tweet) for tweet in tqdm(text_vector, \"Vectorizing...\")]\n","    vectorizer = vectorizer.fit(untokenized_data)\n","    vectors = vectorizer.transform(untokenized_data).toarray()\n","    return vectors\n","  \n","def get_vectors(vectors, labels, keyword):\n","    if len(vectors) != len(labels):\n","        print(\"Unmatching sizes!\")\n","        return\n","    result = list()\n","    for vector, label in zip(vectors, labels):\n","        if label == keyword:\n","            result.append(vector)\n","    return "],"metadata":{"id":"p20fqqGnMxjp","executionInfo":{"status":"ok","timestamp":1684067087723,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sangeeth Ajith","userId":"01644790090705111252"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["vectors_a = tfid(text_vector) # Numerical Vectors A\n","labels_a = labels['category'].values.tolist() # Subtask A Labels\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NyEjCqpwNUY0","executionInfo":{"status":"ok","timestamp":1684067097946,"user_tz":-330,"elapsed":2751,"user":{"displayName":"Sangeeth Ajith","userId":"01644790090705111252"}},"outputId":"e3681cf9-21b4-4c61-f621-b6107b4fbe46"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Vectorizing...: 100%|██████████| 44020/44020 [00:00<00:00, 915835.05it/s]\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","\n","from sklearn.utils import class_weight\n","def compute_class_weight_dictionary(y):\n","    # helper for returning a dictionary instead of an array\n","    classes = np.unique(y)\n","    class_weights = class_weight.compute_class_weight(\"balanced\", classes, y)\n","    class_weight_dict = dict(zip(classes, class_weights))\n","    return class_weight_dict\n","\n","def classify(vectors, labels, type=\"DT\"):\n","    # Random Splitting With Ratio 3 : 1\n","    train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors, labels, random_state=5, test_size=0.2)\n","\n","\n","    class_weights=compute_class_weight_dictionary(train_labels)\n","    #class_weights = class_weight.compute_class_weight('balanced',np.unique(train_labels),train_labels)\n","    # In[25]:\n","\n","    print(class_weights)\n","    # Initialize Model\n","    classifier = None\n","    if(type==\"MNB\"):\n","        classifier = MultinomialNB(alpha=0.7,class_weight=class_weights)\n","        classifier.fit(train_vectors, train_labels)\n","    elif(type==\"KNN\"):\n","        classifier = KNeighborsClassifier(n_jobs=4,class_weight=class_weights)\n","        params = {'n_neighbors': [3,5,7,9], 'weights':['uniform', 'distance']}\n","        classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n","        classifier.fit(train_vectors, train_labels)\n","        classifier = classifier.best_estimator_\n","    elif(type==\"SVM\"):\n","        classifier = SVC(class_weight=class_weights)\n","        classifier = GridSearchCV(classifier, {'C':[0.001, 0.01, 0.1, 1, 10]}, cv=3, n_jobs=4)\n","        classifier.fit(train_vectors, train_labels)\n","        classifier = classifier.best_estimator_\n","    elif(type==\"DT\"):\n","        classifier = DecisionTreeClassifier(max_depth=800, min_samples_split=5,class_weight=class_weights)\n","        params = {'criterion':['gini','entropy']}\n","        classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n","        classifier.fit(train_vectors, train_labels)\n","        classifier = classifier.best_estimator_\n","    elif(type==\"RF\"):\n","        classifier = RandomForestClassifier(max_depth=800, min_samples_split=5,class_weight=class_weights)\n","        params = {'n_estimators': [n for n in range(50,200,50)], 'criterion':['gini','entropy'], }\n","        classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n","        classifier.fit(train_vectors, train_labels)\n","        classifier = classifier.best_estimator_\n","    elif(type==\"LR\"):\n","        classifier = LogisticRegression(multi_class='auto', solver='newton-cg',class_weight=class_weights)\n","        classifier = GridSearchCV(classifier, {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\"]}, cv=3, n_jobs=4)\n","        classifier.fit(train_vectors, train_labels)\n","        classifier = classifier.best_estimator_\n","    else:\n","        print(\"Wrong Classifier Type!\")\n","        return\n","    accuracy = accuracy_score(train_labels, classifier.predict(train_vectors))\n","    print(\"Training Accuracy:\", accuracy)\n","    test_predictions = classifier.predict(test_vectors)\n","    accuracy = accuracy_score(test_labels, test_predictions)\n","    print(\"Test Accuracy:\", accuracy)\n","    print(\"Confusion Matrix:\", )\n","    print(confusion_matrix(test_labels, test_predictions))\n","    print(classification_report([i for i in test_labels], \n","                            [i for i in test_predictions]))"],"metadata":{"id":"5NzNNvR3NWJR","executionInfo":{"status":"ok","timestamp":1684067097948,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sangeeth Ajith","userId":"01644790090705111252"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["accuracy = accuracy_score(train_labels, classifier.predict(train_vectors))\n","    print(\"Training Accuracy:\", accuracy)\n","    test_predictions = classifier.predict(test_vectors)\n","    accuracy = accuracy_score(test_labels, test_predictions)\n","    print(\"Test Accuracy:\", accuracy)\n","    print(\"Confusion Matrix:\", )\n","    print(confusion_matrix(test_labels, test_predictions))\n","    print(classification_report([i for i in test_labels], \n","                            [i for i in test_predictions]))"],"metadata":{"id":"OYGfp9d5Nhc6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\nBuilding Model SVM...\")\n","classify(vectors_a, labels_a, \"SVM\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_65j-3dGNwbG","outputId":"3dd06905-87d7-4dc0-a5ba-779df1d156d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Building Model SVM...\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"U29q3mrTNw7J"},"execution_count":null,"outputs":[]}]}